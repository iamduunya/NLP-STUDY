{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBjBNQX8kQfh"
      },
      "source": [
        "# GPT(Generative Pre-trained Transformer) 2\n",
        "\n",
        "* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKeqNH_dkTmT"
      },
      "source": [
        "* OpenAI에서 GPT 모델 제안\n",
        "* 매우 큰 자연어 처리 데이터를 활용해 비지도 학습으로 사전 학습 후 학습된 가중치를 활용해 파인 튜닝\n",
        "* BERT와 마찬가지로 트랜스포머 모델이지만, BERT는 트랜스포머의 인코더 구조만 사용하고, GPT는 트랜스포머의 디코더 구조(순방향 어텐션)만 사용\n",
        "\n",
        "* GPT2는 GPT1에서 개선되어 레이어 정규화가 부분 블록의 입력쪽에서 사용되고, 셀프 어텐션 이후에 레이어 정규화 적용\n",
        "* GPT2는 GPT1에 비교해 크기가 매우 커진 향상된 모델 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDCr0YqjbfLJ"
      },
      "source": [
        "## 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ixYBCR8bguE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffe4c75-f603-4940-dc98-500c1c75d2d6"
      },
      "source": [
        "!pip install transformers==2.11.0\n",
        "!pip install tensorflow==2.2.0\n",
        "!pip install sentencepiece==0.1.85\n",
        "!pip install gluonnlp==0.9.1\n",
        "!pip install mxnet==1.6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==2.11.0\n",
            "  Using cached transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (1.23.1)\n",
            "Collecting tokenizers==0.7.0 (from transformers==2.11.0)\n",
            "  Using cached tokenizers-0.7.0.tar.gz (81 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (3.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (4.66.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (0.1.99)\n",
            "Collecting sacremoses (from transformers==2.11.0)\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (1.3.2)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.2.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement sentencepiece==0.1.85 (from versions: 0.0.0, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.9, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.83, 0.1.86, 0.1.91, 0.1.92, 0.1.94, 0.1.95, 0.1.96, 0.1.97, 0.1.98, 0.1.99, 0.2.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sentencepiece==0.1.85\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: gluonnlp==0.9.1 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (1.23.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (3.0.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (24.0)\n",
            "Requirement already satisfied: mxnet==1.6.0 in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.6.0) (1.23.1)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.6.0) (2.31.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.6.0) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPhczTnFjsG4"
      },
      "source": [
        "## 데이터 다운로드\n",
        "\n",
        "* https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyCKy2LtjsG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce51a979-b972-4566-d8dc-618a52cc3d13"
      },
      "source": [
        "!mkdir -p gpt2\n",
        "!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt \\\n",
        "      -O gpt2/finetune_data.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-01 09:48:04--  https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24570 (24K) [text/plain]\n",
            "Saving to: ‘gpt2/finetune_data.txt’\n",
            "\n",
            "\rgpt2/finetune_data.   0%[                    ]       0  --.-KB/s               \rgpt2/finetune_data. 100%[===================>]  23.99K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-04-01 09:48:04 (29.6 MB/s) - ‘gpt2/finetune_data.txt’ saved [24570/24570]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install mxnet-mkl==1.6.0 numpy==1.23.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3coqIyQkcAC",
        "outputId": "039db577-35e3-4dcf-c5af-6891d9146c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mxnet-mkl==1.6.0 in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy==1.23.1 in /usr/local/lib/python3.10/dist-packages (1.23.1)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (2.31.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pGgee8pjsHB"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "from transformers import TFGPT2LMHeadModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROOajn6VIzgy"
      },
      "source": [
        "## 사전 학습 모델\n",
        "\n",
        "* https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoGiYGG1jsHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83301b6-78ac-41b2-f4a5-01d5152940d6"
      },
      "source": [
        "!wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n",
        "!unzip -o gpt_ckpt.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-01 09:48:39--  https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.72.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.72.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip [following]\n",
            "--2024-04-01 09:48:40--  https://www.dropbox.com/s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucbc5bc2502ef0abc4c659e014c7.dl.dropboxusercontent.com/cd/0/inline/CQNZyBJpXOvdqrs9PgZNmRwWbdEFM5bEaYrUhh8N1HTZnODKu3xh8WlTI9XS6YPt6AhbP6ZGC5vthMEOHlw3VYhEXB8w17fqrTBky4knyQWaqlE2MnkOTT4LSrEYbfRnG3fXlv6To-FtD6K62D99V9s6/file# [following]\n",
            "--2024-04-01 09:48:40--  https://ucbc5bc2502ef0abc4c659e014c7.dl.dropboxusercontent.com/cd/0/inline/CQNZyBJpXOvdqrs9PgZNmRwWbdEFM5bEaYrUhh8N1HTZnODKu3xh8WlTI9XS6YPt6AhbP6ZGC5vthMEOHlw3VYhEXB8w17fqrTBky4knyQWaqlE2MnkOTT4LSrEYbfRnG3fXlv6To-FtD6K62D99V9s6/file\n",
            "Resolving ucbc5bc2502ef0abc4c659e014c7.dl.dropboxusercontent.com (ucbc5bc2502ef0abc4c659e014c7.dl.dropboxusercontent.com)... 162.125.70.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to ucbc5bc2502ef0abc4c659e014c7.dl.dropboxusercontent.com (ucbc5bc2502ef0abc4c659e014c7.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CQMbnahgIOEvA5fheSMxvCy-PwZf99tP_g0WaS9ZTHWujOPWEqml988wYLGL9KGRmLdU8YzxxIdydqOeR1ZdNpnJmRk3BQX5e_rBoHfx26FEyEQz1xc9BuVTsr4V2QtHlxk7E39icI2W4KnyEAVZ6l5QTDDO-qbOShbcOl9YXebN8JPHNCx3epC01wAPLlOW8BGGNWXUjV47YT-vFVr8Bbd6xraTblz6__7vueDn426otT8BzdatlAux58Sqnf_gfmsATa1l07AfM5IZrVM0Ksc1ROOd31r8cS2AR1kwLCs7PME773xyzIVmO29OcPiIWDlZcEuXoOH-8RshEXdbi8Im1Xw_0bj4c4K4HEklhmHJKJAjMcM-YemVlPsegdmQ8gk/file [following]\n",
            "--2024-04-01 09:48:41--  https://ucbc5bc2502ef0abc4c659e014c7.dl.dropboxusercontent.com/cd/0/inline2/CQMbnahgIOEvA5fheSMxvCy-PwZf99tP_g0WaS9ZTHWujOPWEqml988wYLGL9KGRmLdU8YzxxIdydqOeR1ZdNpnJmRk3BQX5e_rBoHfx26FEyEQz1xc9BuVTsr4V2QtHlxk7E39icI2W4KnyEAVZ6l5QTDDO-qbOShbcOl9YXebN8JPHNCx3epC01wAPLlOW8BGGNWXUjV47YT-vFVr8Bbd6xraTblz6__7vueDn426otT8BzdatlAux58Sqnf_gfmsATa1l07AfM5IZrVM0Ksc1ROOd31r8cS2AR1kwLCs7PME773xyzIVmO29OcPiIWDlZcEuXoOH-8RshEXdbi8Im1Xw_0bj4c4K4HEklhmHJKJAjMcM-YemVlPsegdmQ8gk/file\n",
            "Reusing existing connection to ucbc5bc2502ef0abc4c659e014c7.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460908853 (440M) [application/zip]\n",
            "Saving to: ‘gpt_ckpt.zip’\n",
            "\n",
            "gpt_ckpt.zip        100%[===================>] 439.56M  14.1MB/s    in 26s     \n",
            "\n",
            "2024-04-01 09:49:07 (17.1 MB/s) - ‘gpt_ckpt.zip’ saved [460908853/460908853]\n",
            "\n",
            "Archive:  gpt_ckpt.zip\n",
            "   creating: gpt_ckpt/\n",
            "  inflating: gpt_ckpt/gpt2_kor_tokenizer.spiece  \n",
            "  inflating: gpt_ckpt/config.json    \n",
            "  inflating: gpt_ckpt/tf_model.h5    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fjeaDUNjsHP"
      },
      "source": [
        "class GPT2Model(tf.keras.Model):\n",
        "  def __init__(self, dir_path):\n",
        "    super(GPT2Model, self).__init__()\n",
        "    self.gpt2=TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.gpt2(inputs)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qlmm2I0jsHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8260908-d8ac-4997-c3f8-c4f8cbb66623"
      },
      "source": [
        "BASE_MODE_PATH='./gpt_ckpt'\n",
        "gpt_model=GPT2Model(BASE_MODE_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5ilayG3jsHc"
      },
      "source": [
        "from gluonnlp.vocab import BERTVocab\n",
        "\n",
        "BATCH_SIZE=16\n",
        "NUM_EPOCHS=10\n",
        "MAX_LEN=30\n",
        "TOKENIZER_PATH='./gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer=SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "vocab=nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                       mask_token=None,\n",
        "                                       sep_token=None,\n",
        "                                       cls_token=None,\n",
        "                                       unknown_token='<unk>',\n",
        "                                       padding_token='<pad>',\n",
        "                                       bos_token='<s>',\n",
        "                                       eos_token='</s>'\n",
        "                                       )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaFAxan-jsHg"
      },
      "source": [
        "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=99999):\n",
        "  _logits=logits.numpy()\n",
        "  top_k=min(top_k, logits.shape[-1])\n",
        "  if top_k>0:\n",
        "    indices_to_remove=logits<tf.math.top_k(logits, top_k)[0][...,-1,None]\n",
        "    _logits[indices_to_remove]=filter_value\n",
        "\n",
        "  if top_p>0.0:\n",
        "      sorted_logits=tf.sort(logits,direction='DESCENDING')\n",
        "      sorted_indices=tf.argsort(logits, direction='DESCENDING')\n",
        "      cumulative_probs=tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=1),axis=-1)\n",
        "\n",
        "      sorted_indices_to_remove=cumulative_probs>top_p\n",
        "      sorted_indices_to_remove=tf.concat([False],sorted_indices_to_remove[...,:-1],axis=0)\n",
        "      indices_to_remove=sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
        "\n",
        "      _logits[indices_to_remove]=filter_value\n",
        "  return tf.constant([_logits])\n",
        "\n",
        "def generate_sentence(seed_word, model, max_step=100, greedy=False, top_k=0,top_p=0.):\n",
        "  sentence=seed_word\n",
        "  toked=tokenizer(sentence)\n",
        "\n",
        "  for _ in range(max_step):\n",
        "    input_ids=tf.constant([vocab[vocab.bos_token],]+vocab[toked])[None,:]\n",
        "    outputs=model(input_ids)[:,-1,:]\n",
        "    if greedy:\n",
        "      gen=vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
        "    else:\n",
        "      output_logit=tf_top_k_top_p_filtering(outputs[0],top_k=top_k,top_p=top_p)\n",
        "      gen=vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
        "\n",
        "    if gen=='</s>':\n",
        "      break\n",
        "      sentence+=gen.replace('-',' ')\n",
        "      toked=tokenizer(sentence)\n",
        "\n",
        "    return sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6bhahzWjsHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "36547a79-480e-4ff8-ce0b-34291a364085"
      },
      "source": [
        "generate_sentence('방금', gpt_model, greedy=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9665bc14ca2d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'방금'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-1b09a4d6799d>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[0;34m(seed_word, model, max_step, greedy, top_k, top_p)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mtoked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gluonnlp/data/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         return self._processor.SampleEncodeAsPieces(sample, self._nbest,\n\u001b[0m\u001b[1;32m    562\u001b[0m                                                     self._alpha)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mSampleEncodeAsPieces\u001b[0;34m(self, input, nbest_size, alpha, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSampleEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       return self.Encode(input=input, nbest_size=nbest_size, alpha=alpha,\n\u001b[0m\u001b[1;32m    562\u001b[0m                          out_type=str, enable_sampling=True, **kwargs)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mEncode\u001b[0;34m(self, input, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    501\u001b[0m       if enable_sampling == True and (nbest_size is None or nbest_size == 0 or\n\u001b[1;32m    502\u001b[0m                                       nbest_size == 1 or alpha is None):\n\u001b[0;32m--> 503\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;34m'When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;34m'and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW5jmfiejsHr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5yWJea3I7-n"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVWJaywYjsHw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVXUVGH5jsH0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4fmyXIZJMxm"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDIvpflCjsH5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxW9BUs-jsH9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McrNa1eEjsIC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFJHOJDqjsIG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBYN8CuxjsIK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeP5zGxHjsIN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BZEEq4mIMhr"
      },
      "source": [
        "# GPT2 네이버 영화 리뷰 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXFkRQYxGa0"
      },
      "source": [
        "## 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ijkw_0U2xGa-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNs8XHaUxGbQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcbcRQKwxGbW"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpABh-81xGbW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqPVUEwjxGbb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I6dM15ym7uK"
      },
      "source": [
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IetCxzkbxGbf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_ZCDWgskiRp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVnAFFU-kiny"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF8f3VcJxGbj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuAoVmTGxGbo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w_U2EMQxGbs"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JYb6XjgxGbu"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oUfrW5TxGby"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OsxKKImxGb1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRF8D388xGb5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGj4h0l3xGb9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x-FC6BDxGcB"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKJx63kSxGcF"
      },
      "source": [
        "## 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VywcseLrxGcH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj3dRljzxGcP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}